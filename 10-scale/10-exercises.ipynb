{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXdvvep3n8GN"
   },
   "source": [
    "# DSCI 511: Data acquistion and pre-processing<br>Chapter 10: Scaling for Big Data processing\n",
    "Note: numberings refer to the main notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54y4P3-_stsP"
   },
   "source": [
    "## Additional In-depth Exercises\n",
    "### A. Building a term-document 'matrix' with Spark\n",
    "This is more-so an exercise in using spark, and in general can be accomplished without it, but a term-document matrix, or TDM, is a 2-dimensional ordered array that counts the number of times a word appears in each given document in a collection.\n",
    "\n",
    "In this exercise, your job is to create a TDM with a `pandas` dataframe as the type of output from the different books in this Chapter's `./data/books/` directory.\n",
    "\n",
    "#### 1. Parallelize the Collection of Books\n",
    "Unlike the lecture notes, in this section our goal should be to create an RDD in which each book is a key, value pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4IWw0sauyjq"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhMbvDKDIMdS"
   },
   "source": [
    "#### 2. Count Words for Each Book Using Spark\n",
    "In this part of the problem you should be able to utilize some code from __Sec. 10.4.6__ to create an object like `WordCountTuples` for each file's text in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gsqBiphku1dd"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlnbY8b5Ja-e"
   },
   "source": [
    "#### 3. Group In-Book Word Counts by Word\n",
    "This section should perform a `.GroupByKey()` operation (review spark methods here: https://spark.apache.org/docs/latest/rdd-programming-guide.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gy5lGYB8u3dD"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbOF4GmVJt1n"
   },
   "source": [
    "#### 4. Store in a Dataframe\n",
    "Now that we've produced an RDD with the target data our job is to `collect()` it all and store it in a `pandas` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3ApJf2XvMBS"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "10-exercises.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
